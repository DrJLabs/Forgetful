# mem0-Stack Monitoring & Observability Optimization Guide

## Overview

This guide provides concrete implementation details for optimizing the existing monitoring stack (Prometheus, Grafana, ELK, Jaeger) specifically for autonomous AI agent usage patterns. Focus is on real-time visibility, predictive alerting, and performance insights.

## 1. Prometheus Metrics for AI Agents

### Custom Metrics Implementation

```python
# shared/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, Summary
from functools import wraps
import time
from typing import Dict, Any, Callable

# AI Agent Specific Metrics
ai_memory_operations = Counter(
    'ai_memory_operations_total',
    'Total memory operations by AI agents',
    ['operation', 'agent_type', 'user_id', 'status']
)

ai_memory_latency = Histogram(
    'ai_memory_operation_duration_seconds',
    'Memory operation latency distribution',
    ['operation', 'agent_type'],
    buckets=(0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0, 2.5)
)

ai_context_size = Gauge(
    'ai_agent_context_size_bytes',
    'Current context size per AI agent',
    ['agent_id', 'agent_type']
)

ai_cache_performance = Summary(
    'ai_cache_hit_ratio',
    'Cache hit ratio for AI operations',
    ['cache_layer', 'operation']
)

ai_agent_sessions = Gauge(
    'ai_agent_active_sessions',
    'Number of active AI agent sessions',
    ['agent_type']
)

ai_memory_relevance = Histogram(
    'ai_memory_relevance_score',
    'Distribution of memory relevance scores',
    ['agent_type'],
    buckets=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
)

ai_batch_efficiency = Histogram(
    'ai_batch_operation_size',
    'Batch operation sizes for AI agents',
    ['operation_type'],
    buckets=(1, 5, 10, 20, 50, 100, 200, 500)
)

# Circuit breaker metrics
circuit_breaker_state = Gauge(
    'circuit_breaker_state',
    'Circuit breaker state (0=closed, 1=open, 2=half-open)',
    ['service_name']
)

circuit_breaker_failures = Counter(
    'circuit_breaker_failures_total',
    'Total circuit breaker failures',
    ['service_name', 'error_type']
)

# Degradation metrics
service_degradation_level = Gauge(
    'service_degradation_level',
    'Current degradation level (0=normal, 1=reduced, 2=essential, 3=readonly)',
    ['service_name']
)

class MetricsCollector:
    """Collects and exposes metrics for AI operations"""
    
    def __init__(self):
        self.operation_timers: Dict[str, float] = {}
    
    @staticmethod
    def track_operation(operation: str, agent_type: str = 'unknown'):
        """Decorator to track operation metrics"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                status = 'success'
                user_id = kwargs.get('user_id', 'unknown')
                
                try:
                    result = await func(*args, **kwargs)
                    return result
                except Exception as e:
                    status = 'error'
                    raise e
                finally:
                    # Record metrics
                    duration = time.time() - start_time
                    ai_memory_operations.labels(
                        operation=operation,
                        agent_type=agent_type,
                        user_id=user_id,
                        status=status
                    ).inc()
                    
                    ai_memory_latency.labels(
                        operation=operation,
                        agent_type=agent_type
                    ).observe(duration)
            
            return wrapper
        return decorator
    
    @staticmethod
    def track_cache_hit(cache_layer: str, operation: str, hit: bool):
        """Track cache hit/miss"""
        ai_cache_performance.labels(
            cache_layer=cache_layer,
            operation=operation
        ).observe(1.0 if hit else 0.0)
    
    @staticmethod
    def update_context_size(agent_id: str, agent_type: str, size_bytes: int):
        """Update agent context size metric"""
        ai_context_size.labels(
            agent_id=agent_id,
            agent_type=agent_type
        ).set(size_bytes)
    
    @staticmethod
    def track_relevance_score(agent_type: str, score: float):
        """Track memory relevance scores"""
        ai_memory_relevance.labels(
            agent_type=agent_type
        ).observe(score)
    
    @staticmethod
    def track_batch_size(operation_type: str, size: int):
        """Track batch operation sizes"""
        ai_batch_efficiency.labels(
            operation_type=operation_type
        ).observe(size)
    
    @staticmethod
    def update_circuit_breaker_state(service_name: str, state: str):
        """Update circuit breaker state"""
        state_map = {'closed': 0, 'open': 1, 'half_open': 2}
        circuit_breaker_state.labels(
            service_name=service_name
        ).set(state_map.get(state, -1))
    
    @staticmethod
    def track_circuit_breaker_failure(service_name: str, error_type: str):
        """Track circuit breaker failures"""
        circuit_breaker_failures.labels(
            service_name=service_name,
            error_type=error_type
        ).inc()
    
    @staticmethod
    def update_degradation_level(service_name: str, level: str):
        """Update service degradation level"""
        level_map = {
            'normal': 0,
            'reduced': 1,
            'essential': 2,
            'readonly': 3,
            'maintenance': 4
        }
        service_degradation_level.labels(
            service_name=service_name
        ).set(level_map.get(level, -1))

# FastAPI integration
from fastapi import Request, Response
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
import asyncio

class PrometheusMiddleware:
    """Middleware to collect HTTP metrics"""
    
    def __init__(self, app):
        self.app = app
        
        # HTTP metrics
        self.http_requests = Counter(
            'http_requests_total',
            'Total HTTP requests',
            ['method', 'endpoint', 'status']
        )
        
        self.http_duration = Histogram(
            'http_request_duration_seconds',
            'HTTP request duration',
            ['method', 'endpoint'],
            buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0)
        )
        
        self.concurrent_requests = Gauge(
            'http_concurrent_requests',
            'Number of concurrent HTTP requests'
        )
    
    async def __call__(self, scope, receive, send):
        if scope['type'] != 'http':
            await self.app(scope, receive, send)
            return
        
        path = scope['path']
        method = scope['method']
        
        # Skip metrics endpoint
        if path == '/metrics':
            await self.app(scope, receive, send)
            return
        
        start_time = time.time()
        self.concurrent_requests.inc()
        
        async def send_wrapper(message):
            if message['type'] == 'http.response.start':
                status = message['status']
                duration = time.time() - start_time
                
                # Record metrics
                self.http_requests.labels(
                    method=method,
                    endpoint=path,
                    status=status
                ).inc()
                
                self.http_duration.labels(
                    method=method,
                    endpoint=path
                ).observe(duration)
            
            await send(message)
        
        try:
            await self.app(scope, receive, send_wrapper)
        finally:
            self.concurrent_requests.dec()

# Metrics endpoint
async def metrics_endpoint(request: Request) -> Response:
    """Prometheus metrics endpoint"""
    metrics = generate_latest()
    return Response(content=metrics, media_type=CONTENT_TYPE_LATEST)
```

### AI Pattern Metrics

```python
# shared/monitoring/ai_patterns.py
from prometheus_client import Counter, Histogram, Gauge
import numpy as np
from typing import List, Dict, Any

class AIPatternMetrics:
    """Specialized metrics for AI usage patterns"""
    
    def __init__(self):
        # Query pattern metrics
        self.query_patterns = Counter(
            'ai_query_patterns_total',
            'Query patterns by type',
            ['pattern_type', 'agent_type']
        )
        
        # Memory access patterns
        self.memory_access_patterns = Histogram(
            'ai_memory_access_interval_seconds',
            'Time between memory accesses',
            ['agent_type'],
            buckets=(0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0, 300.0)
        )
        
        # Context evolution metrics
        self.context_growth_rate = Gauge(
            'ai_context_growth_rate_per_hour',
            'Context growth rate per hour',
            ['agent_id']
        )
        
        # Decision quality metrics
        self.decision_confidence = Histogram(
            'ai_decision_confidence_score',
            'AI agent decision confidence distribution',
            ['agent_type', 'decision_type'],
            buckets=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
        )
        
        # Memory consolidation metrics
        self.consolidation_efficiency = Gauge(
            'ai_memory_consolidation_ratio',
            'Ratio of memories consolidated',
            ['agent_type']
        )
        
        # Pattern tracking
        self.pattern_history: Dict[str, List[float]] = {}
    
    def track_query_pattern(self, query: str, agent_type: str):
        """Analyze and track query patterns"""
        pattern_type = self._classify_query_pattern(query)
        self.query_patterns.labels(
            pattern_type=pattern_type,
            agent_type=agent_type
        ).inc()
    
    def track_memory_access(self, agent_type: str, last_access_time: float):
        """Track memory access intervals"""
        current_time = time.time()
        interval = current_time - last_access_time
        
        self.memory_access_patterns.labels(
            agent_type=agent_type
        ).observe(interval)
    
    def calculate_context_growth(self, agent_id: str, context_sizes: List[int]):
        """Calculate context growth rate"""
        if len(context_sizes) < 2:
            return
        
        # Calculate growth rate using linear regression
        times = np.arange(len(context_sizes))
        slope, _ = np.polyfit(times, context_sizes, 1)
        
        # Convert to per-hour rate
        growth_rate = slope * 3600  # Assuming measurements are per second
        
        self.context_growth_rate.labels(
            agent_id=agent_id
        ).set(growth_rate)
    
    def _classify_query_pattern(self, query: str) -> str:
        """Classify query into pattern types"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['error', 'bug', 'issue']):
            return 'debugging'
        elif any(word in query_lower for word in ['implement', 'create', 'build']):
            return 'implementation'
        elif any(word in query_lower for word in ['what', 'how', 'why']):
            return 'exploration'
        elif any(word in query_lower for word in ['fix', 'solve', 'resolve']):
            return 'problem_solving'
        elif any(word in query_lower for word in ['test', 'verify', 'check']):
            return 'validation'
        else:
            return 'general'
```

## 2. Grafana Dashboard Configurations

### AI Agent Operations Dashboard

```json
{
  "dashboard": {
    "title": "AI Agent Memory Operations",
    "uid": "ai-agent-ops",
    "panels": [
      {
        "title": "Memory Operation Latency (P99)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, sum(rate(ai_memory_operation_duration_seconds_bucket[5m])) by (operation, le))",
            "legendFormat": "{{operation}}"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [0.1],
                "type": "gt"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "avg"
              },
              "type": "query"
            }
          ],
          "name": "High Memory Operation Latency"
        }
      },
      {
        "title": "Cache Hit Ratio by Layer",
        "type": "stat",
        "targets": [
          {
            "expr": "avg_over_time(ai_cache_hit_ratio[5m]) by (cache_layer)",
            "legendFormat": "{{cache_layer}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 0.7},
                {"color": "green", "value": 0.85}
              ]
            },
            "unit": "percentunit"
          }
        }
      },
      {
        "title": "Active AI Agent Sessions",
        "type": "bargauge",
        "targets": [
          {
            "expr": "sum(ai_agent_active_sessions) by (agent_type)",
            "legendFormat": "{{agent_type}}"
          }
        ]
      },
      {
        "title": "Circuit Breaker States",
        "type": "state-timeline",
        "targets": [
          {
            "expr": "circuit_breaker_state",
            "legendFormat": "{{service_name}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "mappings": [
              {"text": "Closed", "value": 0, "color": "green"},
              {"text": "Open", "value": 1, "color": "red"},
              {"text": "Half-Open", "value": 2, "color": "yellow"}
            ]
          }
        }
      },
      {
        "title": "Memory Relevance Score Distribution",
        "type": "heatmap",
        "targets": [
          {
            "expr": "sum(increase(ai_memory_relevance_score_bucket[5m])) by (le)",
            "format": "heatmap",
            "legendFormat": "{{le}}"
          }
        ]
      },
      {
        "title": "Batch Operation Efficiency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.5, sum(rate(ai_batch_operation_size_bucket[5m])) by (operation_type, le))",
            "legendFormat": "{{operation_type}} - median"
          }
        ]
      }
    ]
  }
}
```

### System Health Dashboard

```python
# monitoring/grafana/dashboards/system_health.py
import json
from typing import Dict, List, Any

class SystemHealthDashboard:
    """Generate system health dashboard configuration"""
    
    @staticmethod
    def generate_dashboard() -> Dict[str, Any]:
        return {
            "dashboard": {
                "title": "mem0-Stack System Health",
                "uid": "system-health",
                "refresh": "10s",
                "panels": [
                    {
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
                        "title": "Service Degradation Status",
                        "type": "stat",
                        "targets": [{
                            "expr": "service_degradation_level",
                            "legendFormat": "{{service_name}}"
                        }],
                        "fieldConfig": {
                            "defaults": {
                                "mappings": [
                                    {"text": "Normal", "value": 0, "color": "green"},
                                    {"text": "Reduced", "value": 1, "color": "yellow"},
                                    {"text": "Essential", "value": 2, "color": "orange"},
                                    {"text": "Read-Only", "value": 3, "color": "red"},
                                    {"text": "Maintenance", "value": 4, "color": "dark-red"}
                                ],
                                "thresholds": {
                                    "mode": "absolute",
                                    "steps": [
                                        {"color": "green", "value": 0},
                                        {"color": "yellow", "value": 1},
                                        {"color": "orange", "value": 2},
                                        {"color": "red", "value": 3}
                                    ]
                                }
                            }
                        }
                    },
                    {
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
                        "title": "System Resource Usage",
                        "type": "timeseries",
                        "targets": [
                            {
                                "expr": "100 - (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
                                "legendFormat": "CPU Usage %"
                            },
                            {
                                "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
                                "legendFormat": "Memory Usage %"
                            }
                        ],
                        "fieldConfig": {
                            "defaults": {
                                "unit": "percent",
                                "max": 100,
                                "min": 0
                            }
                        }
                    },
                    {
                        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8},
                        "title": "Database Connection Pool Status",
                        "type": "graph",
                        "targets": [
                            {
                                "expr": "pg_stat_activity_count",
                                "legendFormat": "PostgreSQL Active"
                            },
                            {
                                "expr": "pg_stat_activity_max_tx_duration",
                                "legendFormat": "Max Transaction Duration"
                            }
                        ]
                    },
                    {
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16},
                        "title": "Error Rate by Service",
                        "type": "graph",
                        "targets": [{
                            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (endpoint) / sum(rate(http_requests_total[5m])) by (endpoint)",
                            "legendFormat": "{{endpoint}}"
                        }],
                        "alert": {
                            "conditions": [{
                                "evaluator": {
                                    "params": [0.05],
                                    "type": "gt"
                                },
                                "reducer": {
                                    "params": [],
                                    "type": "last"
                                },
                                "type": "query"
                            }],
                            "name": "High Error Rate Alert"
                        }
                    }
                ]
            }
        }
```

## 3. ELK Stack Configuration for AI Patterns

### Logstash Pipeline for AI Events

```ruby
# monitoring/logstash/pipeline/ai_events.conf
input {
  tcp {
    port => 5000
    codec => json_lines
    type => "ai_events"
  }
  
  beats {
    port => 5044
    type => "system_logs"
  }
}

filter {
  # Parse AI agent events
  if [type] == "ai_events" {
    # Extract agent information
    if [agent_id] {
      mutate {
        add_field => {
          "agent_type" => "%{[agent_id][0]}"
          "user_id" => "%{[agent_id][1]}"
        }
      }
    }
    
    # Parse memory operation logs
    if [operation] == "memory_operation" {
      # Calculate operation duration
      ruby {
        code => "
          if event.get('start_time') and event.get('end_time')
            duration = event.get('end_time') - event.get('start_time')
            event.set('duration_ms', duration * 1000)
          end
        "
      }
      
      # Classify operation performance
      if [duration_ms] {
        if [duration_ms] <= 50 {
          mutate { add_field => { "performance_class" => "excellent" } }
        } else if [duration_ms] <= 100 {
          mutate { add_field => { "performance_class" => "good" } }
        } else if [duration_ms] <= 200 {
          mutate { add_field => { "performance_class" => "acceptable" } }
        } else {
          mutate { add_field => { "performance_class" => "poor" } }
        }
      }
    }
    
    # Parse error logs
    if [level] == "ERROR" {
      # Extract error details
      grok {
        match => {
          "message" => "(?<error_type>[A-Za-z]+Error): (?<error_message>.*)"
        }
        tag_on_failure => []
      }
      
      # Add error classification
      if [error_type] {
        mutate {
          add_field => {
            "error_category" => "%{error_type}"
            "requires_attention" => "true"
          }
        }
      }
    }
    
    # Enrich with pattern detection
    if [query] {
      ruby {
        code => "
          query = event.get('query').downcase
          patterns = []
          
          patterns << 'debugging' if query.include?('error') || query.include?('bug')
          patterns << 'implementation' if query.include?('implement') || query.include?('create')
          patterns << 'exploration' if query.include?('what') || query.include?('how')
          
          event.set('query_patterns', patterns) unless patterns.empty?
        "
      }
    }
  }
  
  # Add geographic information if IP is present
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # Calculate metrics for aggregation
  metrics {
    meter => "ai_events"
    add_tag => "metric"
    flush_interval => 60
    rates => [1, 5, 15]
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "ai-events-%{+YYYY.MM.dd}"
    template_name => "ai_events"
    template => "/etc/logstash/templates/ai_events.json"
    template_overwrite => true
  }
  
  # Critical errors to alerting system
  if [requires_attention] == "true" {
    http {
      url => "http://alertmanager:9093/api/v1/alerts"
      http_method => "post"
      format => "json"
      mapping => {
        "labels" => {
          "alertname" => "AIAgentError"
          "severity" => "warning"
          "service" => "mem0-stack"
          "error_type" => "%{error_type}"
        }
        "annotations" => {
          "summary" => "AI Agent Error: %{error_type}"
          "description" => "%{error_message}"
        }
      }
    }
  }
  
  # Performance metrics to monitoring
  if "metric" in [tags] {
    statsd {
      host => "localhost"
      port => 8125
      namespace => "ai_agents"
      sender => "%{agent_type}"
      increment => ["events_processed"]
    }
  }
}
```

### Elasticsearch Index Template

```json
{
  "index_patterns": ["ai-events-*"],
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1,
    "index.refresh_interval": "5s",
    "analysis": {
      "analyzer": {
        "ai_query_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["lowercase", "stop", "snowball"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "@timestamp": {"type": "date"},
      "agent_id": {"type": "keyword"},
      "agent_type": {"type": "keyword"},
      "user_id": {"type": "keyword"},
      "operation": {"type": "keyword"},
      "duration_ms": {"type": "float"},
      "performance_class": {"type": "keyword"},
      "query": {
        "type": "text",
        "analyzer": "ai_query_analyzer",
        "fields": {
          "keyword": {"type": "keyword"}
        }
      },
      "query_patterns": {"type": "keyword"},
      "memory_id": {"type": "keyword"},
      "relevance_score": {"type": "float"},
      "cache_hit": {"type": "boolean"},
      "error_type": {"type": "keyword"},
      "error_message": {"type": "text"},
      "context_size": {"type": "integer"},
      "batch_size": {"type": "integer"}
    }
  }
}
```

## 4. Jaeger Distributed Tracing

### Trace Instrumentation

```python
# shared/monitoring/tracing.py
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.asyncpg import AsyncPGInstrumentor
from contextlib import contextmanager
from typing import Optional, Dict, Any
import time

class TracingManager:
    """Manages distributed tracing for AI operations"""
    
    def __init__(self, service_name: str = "mem0-stack"):
        # Configure Jaeger exporter
        jaeger_exporter = JaegerExporter(
            agent_host_name="localhost",
            agent_port=6831,
            max_tag_value_length=1024  # For large AI contexts
        )
        
        # Create tracer provider
        resource = Resource(attributes={
            "service.name": service_name,
            "service.version": "1.0.0",
            "deployment.environment": "production"
        })
        
        provider = TracerProvider(resource=resource)
        processor = BatchSpanProcessor(jaeger_exporter)
        provider.add_span_processor(processor)
        
        # Set global tracer provider
        trace.set_tracer_provider(provider)
        
        # Get tracer
        self.tracer = trace.get_tracer(__name__)
        
        # Auto-instrument libraries
        FastAPIInstrumentor.instrument()
        RequestsInstrumentor.instrument()
        AsyncPGInstrumentor.instrument()
    
    @contextmanager
    def trace_operation(
        self,
        operation_name: str,
        attributes: Optional[Dict[str, Any]] = None
    ):
        """Context manager for tracing operations"""
        with self.tracer.start_as_current_span(operation_name) as span:
            # Add default attributes
            span.set_attribute("operation.type", "ai_memory")
            
            # Add custom attributes
            if attributes:
                for key, value in attributes.items():
                    span.set_attribute(key, value)
            
            start_time = time.time()
            
            try:
                yield span
            except Exception as e:
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise
            finally:
                # Record duration
                duration = time.time() - start_time
                span.set_attribute("duration_ms", duration * 1000)
    
    def trace_ai_memory_operation(
        self,
        operation_type: str,
        agent_id: str,
        user_id: str
    ):
        """Decorator for tracing AI memory operations"""
        def decorator(func):
            async def wrapper(*args, **kwargs):
                attributes = {
                    "ai.operation": operation_type,
                    "ai.agent_id": agent_id,
                    "ai.user_id": user_id,
                    "ai.context_size": kwargs.get('context_size', 0)
                }
                
                with self.trace_operation(
                    f"ai.memory.{operation_type}",
                    attributes
                ):
                    # Add baggage for downstream propagation
                    span = trace.get_current_span()
                    span.set_attribute("ai.trace_id", span.get_span_context().trace_id)
                    
                    result = await func(*args, **kwargs)
                    
                    # Add result attributes
                    if isinstance(result, dict):
                        span.set_attribute(
                            "ai.result_count",
                            len(result.get('memories', []))
                        )
                        span.set_attribute(
                            "ai.cache_hit",
                            result.get('cached', False)
                        )
                    
                    return result
            
            return wrapper
        return decorator
    
    def create_child_span(
        self,
        name: str,
        attributes: Optional[Dict[str, Any]] = None
    ):
        """Create a child span for nested operations"""
        parent_span = trace.get_current_span()
        
        with self.tracer.start_as_current_span(
            name,
            context=trace.set_span_in_context(parent_span)
        ) as span:
            if attributes:
                for key, value in attributes.items():
                    span.set_attribute(key, value)
            
            return span

# Usage example
tracing_manager = TracingManager()

@tracing_manager.trace_ai_memory_operation(
    "search",
    "coding_agent_123",
    "user_456"
)
async def search_memories_with_tracing(
    query: str,
    limit: int = 10,
    context_size: int = 0
):
    """Example traced memory search"""
    # Trace vector embedding generation
    with tracing_manager.trace_operation(
        "generate_embedding",
        {"query_length": len(query)}
    ):
        embedding = await generate_embedding(query)
    
    # Trace database search
    with tracing_manager.trace_operation(
        "vector_search",
        {"limit": limit}
    ):
        results = await search_vector_db(embedding, limit)
    
    return {"memories": results, "cached": False}
```

## 5. Alerting Rules for AI Agents

### Prometheus Alert Rules

```yaml
# monitoring/prometheus/alerts/ai_agents.yml
groups:
  - name: ai_memory_operations
    interval: 30s
    rules:
      - alert: HighMemoryOperationLatency
        expr: |
          histogram_quantile(0.99, 
            sum(rate(ai_memory_operation_duration_seconds_bucket[5m])) 
            by (operation, le)
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "High memory operation latency detected"
          description: "P99 latency for {{ $labels.operation }} is {{ $value }}s"
          runbook_url: "https://wiki.internal/runbooks/ai-memory-latency"
      
      - alert: LowCacheHitRatio
        expr: |
          avg_over_time(ai_cache_hit_ratio[10m]) < 0.7
        for: 10m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "Cache hit ratio below threshold"
          description: "Cache hit ratio is {{ $value }} for {{ $labels.cache_layer }}"
      
      - alert: CircuitBreakerOpen
        expr: circuit_breaker_state == 1
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Circuit breaker is open"
          description: "Circuit breaker for {{ $labels.service_name }} is open"
      
      - alert: ServiceDegraded
        expr: service_degradation_level > 0
        for: 5m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "Service operating in degraded mode"
          description: "{{ $labels.service_name }} is in degradation level {{ $value }}"
      
      - alert: HighErrorRate
        expr: |
          sum(rate(ai_memory_operations_total{status="error"}[5m])) 
          / sum(rate(ai_memory_operations_total[5m])) > 0.05
        for: 5m
        labels:
          severity: critical
          team: ai-platform
        annotations:
          summary: "High error rate in AI operations"
          description: "Error rate is {{ $value | humanizePercentage }}"
      
      - alert: AIAgentContextGrowthAnomaly
        expr: |
          ai_context_growth_rate_per_hour > 10000
        for: 30m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "Abnormal context growth detected"
          description: "Agent {{ $labels.agent_id }} context growing at {{ $value }} bytes/hour"
      
      - alert: MemoryConsolidationBacklog
        expr: |
          ai_memory_consolidation_ratio < 0.5
        for: 1h
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "Memory consolidation falling behind"
          description: "Only {{ $value | humanizePercentage }} of memories consolidated"

  - name: infrastructure_alerts
    interval: 30s
    rules:
      - alert: DatabaseConnectionPoolExhausted
        expr: |
          pg_stat_activity_count / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "{{ $value | humanizePercentage }} of connections in use"
      
      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{ $value | humanizePercentage }}"
```

### Alertmanager Configuration

```yaml
# monitoring/alertmanager/config.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      continue: true
    
    - match:
        team: ai-platform
      receiver: 'ai-team'
    
    - match:
        team: infrastructure
      receiver: 'infra-team'

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: 'mem0-Stack Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
  
  - name: 'critical-alerts'
    slack_configs:
      - channel: '#critical-alerts'
        title: 'üö® CRITICAL: mem0-Stack Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
  
  - name: 'ai-team'
    slack_configs:
      - channel: '#ai-platform-alerts'
        send_resolved: true
  
  - name: 'infra-team'
    slack_configs:
      - channel: '#infrastructure-alerts'
        send_resolved: true

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']
```

## 6. Custom Monitoring Scripts

### Health Check Script

```python
# monitoring/scripts/ai_health_check.py
import asyncio
import aiohttp
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
import time
from typing import Dict, Any

class AISystemHealthChecker:
    """Comprehensive health checker for AI systems"""
    
    def __init__(self, pushgateway_url: str = "localhost:9091"):
        self.pushgateway_url = pushgateway_url
        self.registry = CollectorRegistry()
        
        # Define gauges
        self.health_score = Gauge(
            'ai_system_health_score',
            'Overall system health score (0-100)',
            registry=self.registry
        )
        
        self.component_health = Gauge(
            'ai_component_health',
            'Individual component health',
            ['component'],
            registry=self.registry
        )
        
        self.check_duration = Gauge(
            'ai_health_check_duration_seconds',
            'Time taken for health check',
            registry=self.registry
        )
    
    async def check_system_health(self) -> Dict[str, Any]:
        """Run comprehensive health checks"""
        start_time = time.time()
        health_results = {}
        
        # Check each component
        checks = [
            ('mem0_api', self._check_mem0_api),
            ('mcp_server', self._check_mcp_server),
            ('postgresql', self._check_postgresql),
            ('neo4j', self._check_neo4j),
            ('redis', self._check_redis),
            ('monitoring', self._check_monitoring)
        ]
        
        for component, check_func in checks:
            try:
                health_results[component] = await check_func()
                self.component_health.labels(component=component).set(
                    100 if health_results[component]['healthy'] else 0
                )
            except Exception as e:
                health_results[component] = {
                    'healthy': False,
                    'error': str(e)
                }
                self.component_health.labels(component=component).set(0)
        
        # Calculate overall health score
        healthy_components = sum(
            1 for r in health_results.values() if r.get('healthy', False)
        )
        total_components = len(health_results)
        overall_score = (healthy_components / total_components) * 100
        
        self.health_score.set(overall_score)
        self.check_duration.set(time.time() - start_time)
        
        # Push to Prometheus
        push_to_gateway(
            self.pushgateway_url,
            job='ai_health_check',
            registry=self.registry
        )
        
        return {
            'overall_score': overall_score,
            'components': health_results,
            'timestamp': time.time()
        }
    
    async def _check_mem0_api(self) -> Dict[str, Any]:
        """Check mem0 API health"""
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(
                    'http://localhost:8000/health',
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    return {
                        'healthy': response.status == 200,
                        'response_time': response.headers.get('X-Process-Time', 'unknown')
                    }
            except Exception as e:
                return {'healthy': False, 'error': str(e)}
    
    async def _check_postgresql(self) -> Dict[str, Any]:
        """Check PostgreSQL health"""
        import asyncpg
        
        try:
            conn = await asyncpg.connect(
                'postgresql://user:password@localhost/mem0',
                timeout=5
            )
            
            # Check vector extension
            result = await conn.fetchval(
                "SELECT count(*) FROM pg_extension WHERE extname = 'vector'"
            )
            
            await conn.close()
            
            return {
                'healthy': True,
                'vector_extension': result > 0
            }
        except Exception as e:
            return {'healthy': False, 'error': str(e)}

# Run health check
async def main():
    checker = AISystemHealthChecker()
    
    while True:
        health = await checker.check_system_health()
        print(f"System Health Score: {health['overall_score']:.1f}%")
        
        for component, status in health['components'].items():
            status_emoji = "‚úÖ" if status['healthy'] else "‚ùå"
            print(f"{status_emoji} {component}: {status}")
        
        await asyncio.sleep(30)  # Check every 30 seconds

if __name__ == "__main__":
    asyncio.run(main())
```

## Conclusion

This monitoring and observability optimization provides:

1. **AI-Specific Metrics** tracking agent patterns, memory operations, and context evolution
2. **Comprehensive Dashboards** for real-time visibility into AI agent behavior
3. **Intelligent Log Processing** with pattern detection and anomaly identification  
4. **Distributed Tracing** for end-to-end request visibility
5. **Proactive Alerting** for autonomous operation issues

The implementation focuses on understanding AI agent behavior patterns, detecting anomalies early, and providing actionable insights for maintaining optimal performance during autonomous operations. 