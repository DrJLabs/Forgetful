[tool:pytest]
# Test discovery
testpaths = tests openmemory/api/tests mem0/tests shared/tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Test execution options
addopts = 
    --verbose
    --tb=short
    --strict-markers
    --strict-config
    --disable-warnings
    --color=yes
    --durations=20
    --failed-first
    --maxfail=5
    --show-capture=no
    --cov=openmemory
    --cov=shared
    --cov=mem0
    --cov-report=html:htmlcov
    --cov-report=term-missing:skip-covered
    --cov-report=xml:coverage.xml
    --cov-report=json:coverage.json
    --cov-fail-under=80
    --cov-branch
    --junit-xml=test-results.xml
    --html=test-report.html
    --self-contained-html

# Test markers
markers =
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, with external dependencies)
    performance: Performance and benchmark tests
    slow: Slow running tests (>5s)
    async_test: Asynchronous tests
    database: Tests that require database connection
    external: Tests that require external services
    mcp: Tests related to MCP protocol
    memory: Tests related to memory operations
    ui: Frontend/UI tests
    api: Backend API tests
    regression: Regression tests
    smoke: Smoke tests (basic functionality)
    security: Security-related tests
    load: Load testing
    e2e: End-to-end tests
    skip_ci: Skip in CI environment
    skip_local: Skip in local development
    requires_docker: Tests requiring Docker services
    requires_network: Tests requiring network access

# Test filtering
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    ignore::UserWarning
    ignore::ResourceWarning
    ignore::FutureWarning
    ignore:.*unclosed.*:ResourceWarning
    ignore:.*coroutine.*was never awaited:RuntimeWarning

# Asyncio configuration
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Timeout settings
timeout = 300
timeout_method = thread

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Log file configuration
log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Test cache
cache_dir = .pytest_cache

# Minimum version requirements
minversion = 7.0

# Additional configuration
console_output_style = progress
python_files_ignore = __pycache__
python_ignore = ["setup.py", "conftest.py"]

# Performance settings
benchmark_group_by = group
benchmark_sort = mean
benchmark_warmup = off
benchmark_disable_gc = true
benchmark_min_rounds = 5
benchmark_max_time = 2.0

# Coverage configuration
[tool:coverage:run]
source = openmemory,shared,mem0
omit = 
    */tests/*
    */test_*.py
    */*_test.py
    */conftest.py
    */venv/*
    */env/*
    */.venv/*
    */node_modules/*
    */migrations/*
    */alembic/*
    */build/*
    */dist/*
    */__pycache__/*
    */.*
    setup.py
    manage.py

[tool:coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod
    @pytest.mark.skip
    @pytest.mark.xfail
    # TYPE_CHECKING
    if TYPE_CHECKING:

[tool:coverage:html]
directory = htmlcov
title = mem0-stack Test Coverage Report

[tool:coverage:xml]
output = coverage.xml

[tool:coverage:json]
output = coverage.json
pretty_print = true 